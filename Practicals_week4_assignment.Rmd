---
title: "QTA Practicals Week 4 Assignment"
date: "24 Oct 2025"
output: html_document
---

### Part 1: Using a dictionary

1. Create a dictionary called `my.dict` that contains three keys: `country`, `law` and `freedom`. Under the country key, put the full name for the US. Under the law key, include any phrase related to "law" or "constitution" (including regex patterns as needed). Under the freedom key, include any phrase related to being "free" or having "liberty" (including regex patterns as needed). 

```{r}
### 
# library(quanteda)
# my.dict <- dictionary(list(country = c("united states", "united states of america"),
#                          law=c("law[a-z]*", "constitution[a-z]*"),
#                         freedom=c("free[a-z]*", "libert[a-z]*")))
```

2. Apply the dictionary to the corpus of inaugural addresses. Be sure to account for possible regex patterns (if needed) and keep a count of every word that was not categorised using the dictionary. If you need to, consult the help docs: <https://quanteda.io/reference/dfm_lookup.html>.

```{r}
### Your answer goes here
```

3. Calculate the percentage of each inaugrual address devoted to themes around freedom (as captured by your simple dictionary). Which two presidents have the highest percent, and which have the lowest?

```{r}
### Your answer goes here
```

### Part 2: Republican versus Democratic inaugural rhetoric

We will use all post-war inaugural addresses to measure which words are most discriminating between Republican and Democratic addresses. 

1. Load the inaugural corpus, subset to only postwar inaugural addresses, tokenise and do some standard preprocessing.

```{r}
### Your answer goes here
```

2. Create an R function that creates contingency table that you will use to calculate the relationship between how often a specific token appears in a speech and whether the speech was delivered by a Republican or Democrat. Make sure that the function returns a _matrix_, not a `quanteda` object.

```{r}
### Your answer goes here
```

3. Begin by focusing on one token, `nation`. Use your function to create a contingency table and print it here.

```{r}
### Your answer goes here
```

4. Now make a hypothetical contingency table assuming that use of the word `nation` is uncorrelated with whether an address is given by a Republican or Democrat (the independence assumption we discussed in lecture).

```{r}
### Your answer goes here
```

5. Calculate the likelihood ratio statistic and the Pearson's $\chi^2$ statistic for the association between frequency of the word `nation` and partisanship of the president. Calculate manually using the formulas and your two contingency tables. Do not use `quanteda`.

```{r}
### Your answer goes here
```

6. Now calculate the likelihood ratio statistic for every feature in the corpus using the `quanteda` function, and plot the top and bottom words. 

```{r}
### Your answer goes here
```

### Part 3: Naïve Bayes

Using the corpus of 2016 US presidential primary candidate tweets, we will now use Naive Bayes to try to classify whether tweets were written by Hillary Clinton or not. 

1. Load the candidate tweets data, and create a DFM using standard pre-processing steps. When you create the corpus, create a new variable indicating whether a tweet was written by Clinton or not.

```{r}
### Your answer goes here
```

2. Create a training set and a validation set. For this exercise, use 75% of your data for training and 25% for validation and name the training set `train` and the validation set `validation`. (Note: we are not going to do cross-validation here, although you should in general!)

```{r}
### Your answer goes here
```

3. Train a Naïve Bayes on the training set after loading the required package.

```{r}
### Your answer goes here
```

4. Create two objects, one called `preds.insample` and one called `preds.validation` where you use the model you just estimated to classify in the training set (in sample) and in the validation set (out of sample).

```{r}
### Your answer goes here
```

5. Create two confusion matrices, one for your in sample classifications and one for your out of sample classifications. Print both confusion matrices.

```{r}
### Your answer goes here
```

6. Create a function that calculates the precision and recall for a specific class $k$, as well as accuracy. Include an argument `insample` which is set to `TRUE` or `FALSE`, and which will indicate whether the performance statistics are in or out of sample.
 
```{r}
### We include the function to compute performance metrics here as an example, but feel free to create one yourself
#precision.recall.k <- function(mytable, k, insample) {
#  i <- which(row.names(mytable) == k)
#  correct.classifications <- mytable[i,i]
#  pred.k <- sum(mytable[,i]) # how many did classifier predict in class k?
#  true.k <- sum(mytable[i,]) # how many are actually in class k (for real)?
  
#  recall <- correct.classifications/true.k
#  precision <- correct.classifications/pred.k
#  accuracy <- sum(diag(mytable)) / sum(mytable)
#  print(mytable)
#  cat("\n", ifelse(insample,"In sample", "Out of sample"), "performance ",
#      "\n  accuracy =", round(accuracy, 2), 
#      "\n",
#      "\n     class =", k,
#      "\n precision =", round(precision, 2), 
#      "\n    recall =", round(recall, 2), "\n")
#}
```

7. Use your function to calculate the performance statistics on the Hillary Clinton class in sample and out of sample. Why is the performance in sample better than the performance out of sample?

```{r}
### Your answer goes here
```

### Part 4: Regularised Regression

Now, let's predict whether a tweet was written by a Democratic candidate or a Republican candidate using regularised regression.

1. Recreate the tweet DFM including a variable for whether a tweet was written by a Democratic or Republican candidate.

```{r}
### Your answer goes here
```

2. Create a new training set and a validation set. This time, use 85% of your data for training and 15% for validation.

```{r}
### Your answer goes here
```

3. Train a lasso regression on the training set to predict the partisanship of a tweet's author. You should use 10-fold cross validation.

```{r}
### Your answer goes here
```

4. Again predict in sample and out of sample and create two corresponding confusion matrices. 

```{r}
### Your answer goes here
```

5. Calculate the performance metrics for whether the classifier correctly predicts whether a Democrat wrote a tweet both in sample and out of sample. Use your function from above. Did the Naïve Bayes or the Lasso regression do better?

```{r}
### Your answer goes here
```
